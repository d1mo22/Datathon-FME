{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0985ac6a",
   "metadata": {},
   "source": [
    "# üéØ Pipeline Optimizado - Datathon FME\n",
    "\n",
    "## Objetivo\n",
    "Predecir **iap_revenue_d7** (revenue de usuarios que vieron un ad) usando un modelo two-stage:\n",
    "1. **Clasificador**: Predice si el usuario comprar√° (buyer_d7)\n",
    "2. **Regresor**: Predice el revenue solo para compradores\n",
    "\n",
    "## üìã Instrucciones de Uso\n",
    "\n",
    "### Ejecutar en orden:\n",
    "1. **Celda 3**: Cargar datos de train con Dask\n",
    "2. **Celda 5**: Procesar train (feature engineering + object parsing) ‚Üí Guardar en disco\n",
    "3. **Celda 7**: Preprocessing avanzado (binning, interacciones, encoding)\n",
    "4. **Celda 9**: Entrenar modelo two-stage con LightGBM\n",
    "5. **Celda 11**: Procesar test set\n",
    "6. **Celda 12**: Generar predicciones y submission.csv\n",
    "\n",
    "## üîß Preprocessing Incluido\n",
    "\n",
    "### Fase 1: Feature Engineering B√°sico\n",
    "- Ratios de actividad (days_active_ratio, sessions_per_week)\n",
    "- Flags de comportamiento (is_new_user, is_veteran_user, recent_buyer)\n",
    "- M√©tricas de engagement (total_time_in_app)\n",
    "\n",
    "### Fase 2: Extracci√≥n de Object Columns\n",
    "- Stats de arrays/dicts: count, sum, max, min, mean, std\n",
    "- Top 7 columnas prioritarias relacionadas con revenue\n",
    "\n",
    "### Fase 3: Preprocessing Avanzado\n",
    "- **Binning**: Discretizaci√≥n de variables continuas (user_age, sessions, session_time)\n",
    "- **Interacciones**: Features combinadas (engagement_score, avg_revenue_per_purchase)\n",
    "- **Target Encoding**: Encoding con target para categ√≥ricas de baja cardinalidad\n",
    "- **Frequency Encoding**: Encoding por frecuencia para top 5 categ√≥ricas\n",
    "\n",
    "## üíæ Optimizaci√≥n de Memoria\n",
    "- Procesamiento **lazy** con Dask (no carga todo en RAM)\n",
    "- Muestreo inteligente (50% por defecto)\n",
    "- Persistencia en disco (Parquet comprimido)\n",
    "- RAM necesaria: **~4-8GB** (vs 26GB+ anterior)\n",
    "\n",
    "## ‚öôÔ∏è Ajustes\n",
    "- Cambiar rango de fechas: Modificar `filters` en celda 3\n",
    "- Ajustar muestra: Cambiar `sample_fraction` en celda 7\n",
    "- M√°s columnas object: Editar `top_priority_cols` en celda 5\n",
    "- Modelos guardados en: `./models/`\n",
    "- Submission guardado en: `./outputs/submission.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e51218",
   "metadata": {},
   "source": [
    "# üìÇ PASO 1: Cargar Datos de Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6e07f03-8332-451d-976d-f163a62bc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/monto/.local/lib/python3.11/site-packages (22.0.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install dask\n",
    "#!pip install pyarrow\n",
    "!pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a8c2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "\n",
    "dataset_path = \"./smadex-challenge-predict-the-revenue/train/train\"\n",
    "filters = [(\"datetime\", \">=\", \"2025-10-06-00-00\"), (\"datetime\", \"<\", \"2025-10-20-00-00\")]\n",
    "\n",
    "ddf = dd.read_parquet(\n",
    "    dataset_path,\n",
    "    filters=filters\n",
    ")\n",
    "\n",
    "ddf = ddf.drop(columns=['buy_d14', 'buy_d28', 'buy_d7', 'buyer_d1', 'buyer_d14', 'buyer_d28', \n",
    "                        'iap_revenue_d28', 'iap_revenue_d14', \n",
    "                        'registration', 'retention_d1', 'retention_d1_to_d7', 'retention_d3',\n",
    "                        'retention_d3_to_d7', 'retention_d7_to_d14', 'retentiond7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3d01011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buyer_d7</th>\n",
       "      <th>iap_revenue_d7</th>\n",
       "      <th>advertiser_bundle</th>\n",
       "      <th>advertiser_category</th>\n",
       "      <th>advertiser_subcategory</th>\n",
       "      <th>advertiser_bottom_taxonomy_level</th>\n",
       "      <th>carrier</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>dev_make</th>\n",
       "      <th>dev_model</th>\n",
       "      <th>dev_os</th>\n",
       "      <th>dev_osv</th>\n",
       "      <th>hour</th>\n",
       "      <th>release_date</th>\n",
       "      <th>release_msrp</th>\n",
       "      <th>weekday</th>\n",
       "      <th>avg_act_days</th>\n",
       "      <th>avg_daily_sessions</th>\n",
       "      <th>avg_days_ins</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>bcat</th>\n",
       "      <th>bcat_bottom_taxonomy</th>\n",
       "      <th>bundles_cat</th>\n",
       "      <th>bundles_cat_bottom_taxonomy</th>\n",
       "      <th>bundles_ins</th>\n",
       "      <th>city_hist</th>\n",
       "      <th>country_hist</th>\n",
       "      <th>cpm</th>\n",
       "      <th>cpm_pct_rk</th>\n",
       "      <th>ctr</th>\n",
       "      <th>ctr_pct_rk</th>\n",
       "      <th>dev_language_hist</th>\n",
       "      <th>dev_osv_hist</th>\n",
       "      <th>first_request_ts</th>\n",
       "      <th>first_request_ts_bundle</th>\n",
       "      <th>first_request_ts_category_bottom_taxonomy</th>\n",
       "      <th>hour_ratio</th>\n",
       "      <th>iap_revenue_usd_bundle</th>\n",
       "      <th>iap_revenue_usd_category</th>\n",
       "      <th>iap_revenue_usd_category_bottom_taxonomy</th>\n",
       "      <th>last_buy</th>\n",
       "      <th>last_buy_ts_bundle</th>\n",
       "      <th>last_buy_ts_category</th>\n",
       "      <th>last_ins</th>\n",
       "      <th>last_install_ts_bundle</th>\n",
       "      <th>last_install_ts_category</th>\n",
       "      <th>advertiser_actions_action_count</th>\n",
       "      <th>advertiser_actions_action_last_timestamp</th>\n",
       "      <th>user_actions_bundles_action_count</th>\n",
       "      <th>user_actions_bundles_action_last_timestamp</th>\n",
       "      <th>last_advertiser_action</th>\n",
       "      <th>new_bundles</th>\n",
       "      <th>num_buys_bundle</th>\n",
       "      <th>num_buys_category</th>\n",
       "      <th>num_buys_category_bottom_taxonomy</th>\n",
       "      <th>region_hist</th>\n",
       "      <th>rev_by_adv</th>\n",
       "      <th>rwd_prank</th>\n",
       "      <th>user_bundles</th>\n",
       "      <th>user_bundles_l28d</th>\n",
       "      <th>weekend_ratio</th>\n",
       "      <th>weeks_since_first_seen</th>\n",
       "      <th>wifi_ratio</th>\n",
       "      <th>whale_users_bundle_num_buys_prank</th>\n",
       "      <th>whale_users_bundle_revenue_prank</th>\n",
       "      <th>whale_users_bundle_total_num_buys</th>\n",
       "      <th>whale_users_bundle_total_revenue</th>\n",
       "      <th>row_id</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=24</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>category[known]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: drop_by_shallow_copy, 2 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               buyer_d7 iap_revenue_d7 advertiser_bundle advertiser_category advertiser_subcategory advertiser_bottom_taxonomy_level carrier country  region dev_make dev_model  dev_os dev_osv    hour release_date release_msrp weekday avg_act_days avg_daily_sessions avg_days_ins avg_duration    bcat bcat_bottom_taxonomy bundles_cat bundles_cat_bottom_taxonomy bundles_ins city_hist country_hist     cpm cpm_pct_rk     ctr ctr_pct_rk dev_language_hist dev_osv_hist first_request_ts first_request_ts_bundle first_request_ts_category_bottom_taxonomy hour_ratio iap_revenue_usd_bundle iap_revenue_usd_category iap_revenue_usd_category_bottom_taxonomy last_buy last_buy_ts_bundle last_buy_ts_category last_ins last_install_ts_bundle last_install_ts_category advertiser_actions_action_count advertiser_actions_action_last_timestamp user_actions_bundles_action_count user_actions_bundles_action_last_timestamp last_advertiser_action new_bundles num_buys_bundle num_buys_category num_buys_category_bottom_taxonomy region_hist rev_by_adv rwd_prank user_bundles user_bundles_l28d weekend_ratio weeks_since_first_seen wifi_ratio whale_users_bundle_num_buys_prank whale_users_bundle_revenue_prank whale_users_bundle_total_num_buys whale_users_bundle_total_revenue  row_id         datetime\n",
       "npartitions=24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "                  int32        float64            object              object                 object                           object  object  object  object   object    object  object  object  object       object        int64   int32      float64             object      float64       object  object               object      object                      object      object    object       object  object     object  object     object            object       object            int64                  object                                    object     object                 object                   object                                   object    int64             object               object    int64                 object                   object                          object                                   object                            object                                     object                 object      object          object            object                            object      object     object    object       object            object       float64                  int32    float64                            object                           object                            object                           object  object  category[known]\n",
       "                    ...            ...               ...                 ...                    ...                              ...     ...     ...     ...      ...       ...     ...     ...     ...          ...          ...     ...          ...                ...          ...          ...     ...                  ...         ...                         ...         ...       ...          ...     ...        ...     ...        ...               ...          ...              ...                     ...                                       ...        ...                    ...                      ...                                      ...      ...                ...                  ...      ...                    ...                      ...                             ...                                      ...                               ...                                        ...                    ...         ...             ...               ...                               ...         ...        ...       ...          ...               ...           ...                    ...        ...                               ...                              ...                               ...                              ...     ...              ...\n",
       "...                 ...            ...               ...                 ...                    ...                              ...     ...     ...     ...      ...       ...     ...     ...     ...          ...          ...     ...          ...                ...          ...          ...     ...                  ...         ...                         ...         ...       ...          ...     ...        ...     ...        ...               ...          ...              ...                     ...                                       ...        ...                    ...                      ...                                      ...      ...                ...                  ...      ...                    ...                      ...                             ...                                      ...                               ...                                        ...                    ...         ...             ...               ...                               ...         ...        ...       ...          ...               ...           ...                    ...        ...                               ...                              ...                               ...                              ...     ...              ...\n",
       "                    ...            ...               ...                 ...                    ...                              ...     ...     ...     ...      ...       ...     ...     ...     ...          ...          ...     ...          ...                ...          ...          ...     ...                  ...         ...                         ...         ...       ...          ...     ...        ...     ...        ...               ...          ...              ...                     ...                                       ...        ...                    ...                      ...                                      ...      ...                ...                  ...      ...                    ...                      ...                             ...                                      ...                               ...                                        ...                    ...         ...             ...               ...                               ...         ...        ...       ...          ...               ...           ...                    ...        ...                               ...                              ...                               ...                              ...     ...              ...\n",
       "                    ...            ...               ...                 ...                    ...                              ...     ...     ...     ...      ...       ...     ...     ...     ...          ...          ...     ...          ...                ...          ...          ...     ...                  ...         ...                         ...         ...       ...          ...     ...        ...     ...        ...               ...          ...              ...                     ...                                       ...        ...                    ...                      ...                                      ...      ...                ...                  ...      ...                    ...                      ...                             ...                                      ...                               ...                                        ...                    ...         ...             ...               ...                               ...         ...        ...       ...          ...               ...           ...                    ...        ...                               ...                              ...                               ...                              ...     ...              ...\n",
       "Dask Name: drop_by_shallow_copy, 2 expressions\n",
       "Expr=Drop(frame=ReadParquetFSSpec(46ee016), columns=['buy_d14', 'buy_d28', 'buy_d7', 'buyer_d1', 'buyer_d14', 'buyer_d28', 'iap_revenue_d28', 'iap_revenue_d14', 'registration', 'retention_d1', 'retention_d1_to_d7', 'retention_d3', 'retention_d3_to_d7', 'retention_d7_to_d14', 'retentiond7'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545cb39c",
   "metadata": {},
   "source": [
    "# üîß PASO 2: Procesar Train y Guardar en Disco\n",
    "\n",
    "Este paso:\n",
    "- Aplica feature engineering (ratios, flags)\n",
    "- Extrae features de columnas object (top 5 prioritarias)\n",
    "- Guarda resultado en `./processed_train_data/`\n",
    "- **NO carga todo en RAM** (procesamiento lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6ddc6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESAMIENTO CON DASK (SIN CARGAR TODO EN RAM)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Feature engineering b√°sico (lazy)...\n",
      "‚úì 12+ features de engagement a√±adidas (lazy)\n",
      "‚úì Particiones actuales: 24\n",
      "\n",
      "[2/6] Procesando columnas object...\n",
      "Procesando 7 columnas object...\n",
      "  ‚úì iap_revenue_usd_bundle ‚Üí 6 features\n",
      "  ‚úì num_buys_bundle ‚Üí 6 features\n",
      "  ‚úì whale_users_bundle_total_revenue ‚Üí 6 features\n",
      "  ‚úì user_bundles ‚Üí 6 features\n",
      "  ‚úì country_hist ‚Üí 6 features\n",
      "  ‚úì advertiser_actions_action_count ‚Üí 6 features\n",
      "  ‚úì user_actions_bundles_action_count ‚Üí 6 features\n",
      "\n",
      "‚úì Features object procesadas (lazy)\n",
      "\n",
      "[3/6] Guardando datos procesados en disco...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Guardar como parquet (comprimido)\u001b[39;00m\n\u001b[32m    157\u001b[39m output_path = \u001b[33m'\u001b[39m\u001b[33m./processed_train_data\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[43mddf_numeric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msnappy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Datos guardados en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Columnas guardadas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(numeric_cols_dask)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:3321\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m   3318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, **kwargs):\n\u001b[32m   3319\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdask_expr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dask/dataframe/dask_expr/io/parquet.py:594\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\u001b[39m\n\u001b[32m    585\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUser-defined key/value metadata (custom_metadata) can not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontain a b\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m'\u001b[39m\u001b[33m key.  This key is reserved by Pandas, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand overwriting the corresponding value can render the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mentire dataset unreadable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    590\u001b[39m     )\n\u001b[32m    592\u001b[39m \u001b[38;5;66;03m# Engine-specific initialization steps to write the dataset.\u001b[39;00m\n\u001b[32m    593\u001b[39m \u001b[38;5;66;03m# Possibly create parquet metadata, and load existing stuff if appending\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m i_offset, fmd, metadata_file_exists, extra_write_kwargs = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_divisions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdivision_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdivision_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[38;5;66;03m# By default we only write a metadata file when appending if one already\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# exists\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m append \u001b[38;5;129;01mand\u001b[39;00m write_metadata_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:654\u001b[39m, in \u001b[36mArrowDatasetEngine.initialize_write\u001b[39m\u001b[34m(cls, df, fs, path, append, partition_on, ignore_divisions, division_info, schema, index_cols, **kwargs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize_write\u001b[39m(\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m     **kwargs,\n\u001b[32m    651\u001b[39m ):\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m schema == \u001b[33m\"\u001b[39m\u001b[33minfer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    653\u001b[39m         \u001b[38;5;66;03m# Start with schema from _meta_nonempty\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m         inferred_schema = \u001b[43mpyarrow_schema_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_meta_nonempty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex_cols\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_meta_nonempty\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.remove_metadata()\n\u001b[32m    660\u001b[39m         \u001b[38;5;66;03m# Use dict to update our inferred schema\u001b[39;00m\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dask/utils.py:781\u001b[39m, in \u001b[36mDispatch.__call__\u001b[39m\u001b[34m(self, arg, *args, **kwargs)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    778\u001b[39m \u001b[33;03mCall the corresponding method based on type of argument.\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    780\u001b[39m meth = \u001b[38;5;28mself\u001b[39m.dispatch(\u001b[38;5;28mtype\u001b[39m(arg))\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/dask/dataframe/backends.py:240\u001b[39m, in \u001b[36mget_pyarrow_schema_pandas\u001b[39m\u001b[34m(obj, preserve_index)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;129m@pyarrow_schema_dispatch\u001b[39m.register((pd.DataFrame,))\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_pyarrow_schema_pandas\u001b[39m(obj, preserve_index=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m.Schema.from_pandas(obj, preserve_index=preserve_index)\n",
      "\u001b[31mNameError\u001b[39m: name 'pa' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VERSI√ìN DASK - PREPROCESSING SIN COMPUTE\n",
    "# ============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESAMIENTO CON DASK (SIN CARGAR TODO EN RAM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "# IMPORTANTE: NO hacer .compute() hasta el final\n",
    "\n",
    "# ============================================\n",
    "# PASO 1: FEATURE ENGINEERING EN DASK (LAZY)\n",
    "# ============================================\n",
    "print(\"\\n[1/6] Feature engineering b√°sico (lazy)...\")\n",
    "\n",
    "# 1.1 Features de ratio y proporciones\n",
    "if 'avg_act_days' in ddf.columns and 'weeks_since_first_seen' in ddf.columns:\n",
    "    ddf['days_active_ratio'] = ddf['avg_act_days'] / (ddf['weeks_since_first_seen'] * 7 + 1e-10)\n",
    "\n",
    "if 'weekend_ratio' in ddf.columns:\n",
    "    ddf['is_weekend_user'] = (ddf['weekend_ratio'] > 0.5).astype(int)\n",
    "\n",
    "if 'wifi_ratio' in ddf.columns:\n",
    "    ddf['is_wifi_user'] = (ddf['wifi_ratio'] > 0.7).astype(int)\n",
    "\n",
    "# 1.2 Features de engagement temporal\n",
    "if 'weeks_since_first_seen' in ddf.columns:\n",
    "    ddf['user_age_days'] = ddf['weeks_since_first_seen'] * 7\n",
    "    ddf['is_new_user'] = (ddf['weeks_since_first_seen'] < 1).astype(int)\n",
    "    ddf['is_veteran_user'] = (ddf['weeks_since_first_seen'] > 12).astype(int)\n",
    "\n",
    "# 1.3 Features de compra hist√≥rica\n",
    "if 'last_buy' in ddf.columns:\n",
    "    ddf['has_previous_purchase'] = (ddf['last_buy'] > 0).astype(int)\n",
    "    ddf['days_since_last_buy'] = ddf['last_buy']\n",
    "    ddf['recent_buyer'] = (ddf['last_buy'] < 7).astype(int)\n",
    "\n",
    "# 1.4 Features de sesiones\n",
    "if 'total_sessions' in ddf.columns and 'weeks_since_first_seen' in ddf.columns:\n",
    "    ddf['avg_sessions_per_week'] = ddf['total_sessions'] / (ddf['weeks_since_first_seen'] + 1)\n",
    "\n",
    "if 'avg_session_time' in ddf.columns and 'total_sessions' in ddf.columns:\n",
    "    ddf['total_time_in_app'] = ddf['avg_session_time'] * ddf['total_sessions']\n",
    "\n",
    "print(\"‚úì 12+ features de engagement a√±adidas (lazy)\")\n",
    "print(f\"‚úì Particiones actuales: {ddf.npartitions}\")\n",
    "\n",
    "# ============================================\n",
    "# PASO 2: PROCESAR COLUMNAS OBJECT (STATS AVANZADAS)\n",
    "# ============================================\n",
    "print(\"\\n[2/6] Procesando columnas object...\")\n",
    "\n",
    "def extract_array_stats_advanced(series, prefix):\n",
    "    \"\"\"\n",
    "    Extrae stats completas de arrays/dicts con informaci√≥n m√°s rica\n",
    "    \"\"\"\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pyarrow as pa\n",
    "    \n",
    "    def safe_parse(x):\n",
    "        try:\n",
    "            if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "                return []\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            parsed = ast.literal_eval(str(x))\n",
    "            if isinstance(parsed, (list, dict)):\n",
    "                if isinstance(parsed, dict):\n",
    "                    return list(parsed.values())\n",
    "                return parsed\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "    \n",
    "    def get_numeric_values(arr):\n",
    "        nums = []\n",
    "        for val in arr:\n",
    "            try:\n",
    "                if val is not None and not (isinstance(val, float) and np.isnan(val)):\n",
    "                    nums.append(float(val))\n",
    "            except:\n",
    "                pass\n",
    "        return nums\n",
    "    \n",
    "    parsed = series.apply(safe_parse, meta=('x', 'object'))\n",
    "    \n",
    "    # Stats b√°sicas\n",
    "    count = parsed.apply(len, meta=('x', 'int64'))\n",
    "    numeric_vals = parsed.apply(get_numeric_values, meta=('x', 'object'))\n",
    "    \n",
    "    # Stats agregadas\n",
    "    def safe_sum(x): return sum(x) if x else 0\n",
    "    def safe_max(x): return max(x) if x else 0\n",
    "    def safe_min(x): return min(x) if x else 0\n",
    "    def safe_mean(x): return np.mean(x) if x else 0\n",
    "    def safe_std(x): return np.std(x) if len(x) > 1 else 0\n",
    "    \n",
    "    total_sum = numeric_vals.apply(safe_sum, meta=('x', 'float64'))\n",
    "    total_max = numeric_vals.apply(safe_max, meta=('x', 'float64'))\n",
    "    total_min = numeric_vals.apply(safe_min, meta=('x', 'float64'))\n",
    "    total_mean = numeric_vals.apply(safe_mean, meta=('x', 'float64'))\n",
    "    total_std = numeric_vals.apply(safe_std, meta=('x', 'float64'))\n",
    "    \n",
    "    return count, total_sum, total_max, total_min, total_mean, total_std\n",
    "\n",
    "# Columnas prioritarias (revenue-related)\n",
    "top_priority_cols = [\n",
    "    'iap_revenue_usd_bundle',\n",
    "    'num_buys_bundle',\n",
    "    'whale_users_bundle_total_revenue',\n",
    "    'user_bundles',\n",
    "    'country_hist',\n",
    "    'advertiser_actions_action_count',\n",
    "    'user_actions_bundles_action_count'\n",
    "]\n",
    "\n",
    "cols_to_process = [col for col in top_priority_cols if col in ddf.columns]\n",
    "\n",
    "print(f\"Procesando {len(cols_to_process)} columnas object...\")\n",
    "\n",
    "for col in cols_to_process:\n",
    "    try:\n",
    "        count, total_sum, total_max, total_min, total_mean, total_std = extract_array_stats_advanced(ddf[col], col)\n",
    "        \n",
    "        ddf[f'{col}_count'] = count\n",
    "        ddf[f'{col}_sum'] = total_sum\n",
    "        ddf[f'{col}_max'] = total_max\n",
    "        ddf[f'{col}_min'] = total_min\n",
    "        ddf[f'{col}_mean'] = total_mean\n",
    "        ddf[f'{col}_std'] = total_std\n",
    "        \n",
    "        print(f\"  ‚úì {col} ‚Üí 6 features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  {col} ‚Üí Error: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\n‚úì Features object procesadas (lazy)\")\n",
    "\n",
    "# ============================================\n",
    "# PASO 3: GUARDAR DATOS PROCESADOS EN DISCO\n",
    "# ============================================\n",
    "print(\"\\n[3/6] Guardando datos procesados en disco...\")\n",
    "\n",
    "# Seleccionar solo columnas num√©ricas + target\n",
    "numeric_cols_dask = ddf.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "\n",
    "# Eliminar columnas object para reducir tama√±o\n",
    "ddf_numeric = ddf[numeric_cols_dask]\n",
    "\n",
    "# Guardar como parquet (comprimido)\n",
    "output_path = './processed_train_data'\n",
    "ddf_numeric.to_parquet(output_path, compression='snappy', overwrite=True, engine='pyarrow')\n",
    "\n",
    "print(f\"‚úì Datos guardados en: {output_path}\")\n",
    "print(f\"‚úì Columnas guardadas: {len(numeric_cols_dask)}\")\n",
    "\n",
    "# Liberar memoria\n",
    "del ddf\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n‚úì Memoria liberada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d51c2c",
   "metadata": {},
   "source": [
    "# üß™ PASO 3: Preprocessing Avanzado\n",
    "\n",
    "- Carga datos procesados (50% sample)\n",
    "- **Binning**: Discretiza variables continuas\n",
    "- **Interacciones**: Crea features combinadas\n",
    "- **Target Encoding**: Encode categ√≥ricas con target\n",
    "- **Frequency Encoding**: Encode frecuencia de categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400cfcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING AVANZADO (Target Encoding, Binning, Agregaciones)\n",
      "================================================================================\n",
      "\n",
      "[1/5] Cargando datos procesados...\n",
      "\n",
      "[1/5] Cargando datos procesados...\n",
      "‚úì Datos cargados: (1653239, 63)\n",
      "‚úì Datos cargados: (1653239, 63)\n",
      "\n",
      "[2/5] Aplicando binning a variables continuas...\n",
      "‚úì 1 variables binned\n",
      "\n",
      "[3/5] Creando features de interacci√≥n...\n",
      "‚úì 2 features de interacci√≥n creadas\n",
      "\n",
      "[4/5] Aplicando target encoding a variables categ√≥ricas...\n",
      "‚úì No hay columnas categ√≥ricas para encodear\n",
      "\n",
      "[5/5] Aplicando frequency encoding...\n",
      "‚úì 0 columnas frequency-encoded\n",
      "\n",
      "================================================================================\n",
      "RESUMEN DE PREPROCESSING\n",
      "================================================================================\n",
      "Features originales:     61\n",
      "+ Binning:               1\n",
      "+ Interacciones:         2\n",
      "+ Target encoding:       0\n",
      "+ Frequency encoding:    0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Total features finales:  64\n",
      "Total samples:           1,653,239\n",
      "================================================================================\n",
      "\n",
      "[2/5] Aplicando binning a variables continuas...\n",
      "‚úì 1 variables binned\n",
      "\n",
      "[3/5] Creando features de interacci√≥n...\n",
      "‚úì 2 features de interacci√≥n creadas\n",
      "\n",
      "[4/5] Aplicando target encoding a variables categ√≥ricas...\n",
      "‚úì No hay columnas categ√≥ricas para encodear\n",
      "\n",
      "[5/5] Aplicando frequency encoding...\n",
      "‚úì 0 columnas frequency-encoded\n",
      "\n",
      "================================================================================\n",
      "RESUMEN DE PREPROCESSING\n",
      "================================================================================\n",
      "Features originales:     61\n",
      "+ Binning:               1\n",
      "+ Interacciones:         2\n",
      "+ Target encoding:       0\n",
      "+ Frequency encoding:    0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Total features finales:  64\n",
      "Total samples:           1,653,239\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PASO 3.5: CARGAR Y APLICAR PREPROCESSING AVANZADO\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING AVANZADO (Target Encoding, Binning, Agregaciones)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos procesados\n",
    "print(\"\\n[1/5] Cargando datos procesados...\")\n",
    "ddf_processed = dd.read_parquet('./processed_train_data')\n",
    "\n",
    "sample_fraction = 0.5\n",
    "ddf_sample = ddf_processed.sample(frac=sample_fraction, random_state=42)\n",
    "df_train = ddf_sample.compute()\n",
    "\n",
    "print(f\"‚úì Datos cargados: {df_train.shape}\")\n",
    "\n",
    "# Separar targets\n",
    "y_buyer = df_train['buyer_d7'] if 'buyer_d7' in df_train.columns else None\n",
    "y_revenue = df_train['iap_revenue_d7'] if 'iap_revenue_d7' in df_train.columns else None\n",
    "X_base = df_train.drop(columns=['buyer_d7', 'iap_revenue_d7'], errors='ignore')\n",
    "\n",
    "# ============================================\n",
    "# PREPROCESSING AVANZADO INCREMENTAL\n",
    "# ============================================\n",
    "\n",
    "# 4/6: BINNING de variables continuas\n",
    "print(\"\\n[2/5] Aplicando binning a variables continuas...\")\n",
    "\n",
    "binning_features = {}\n",
    "\n",
    "# Binning para engagement metrics\n",
    "if 'weeks_since_first_seen' in X_base.columns:\n",
    "    X_base['user_age_bin'] = pd.qcut(X_base['weeks_since_first_seen'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n",
    "    binning_features['user_age_bin'] = True\n",
    "\n",
    "if 'total_sessions' in X_base.columns:\n",
    "    X_base['sessions_bin'] = pd.qcut(X_base['total_sessions'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n",
    "    binning_features['sessions_bin'] = True\n",
    "\n",
    "if 'avg_session_time' in X_base.columns:\n",
    "    X_base['session_time_bin'] = pd.qcut(X_base['avg_session_time'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n",
    "    binning_features['session_time_bin'] = True\n",
    "\n",
    "print(f\"‚úì {len(binning_features)} variables binned\")\n",
    "\n",
    "# 5/6: FEATURES DE INTERACCI√ìN\n",
    "print(\"\\n[3/5] Creando features de interacci√≥n...\")\n",
    "\n",
    "interaction_count = 0\n",
    "\n",
    "# Interacciones de engagement\n",
    "if 'total_sessions' in X_base.columns and 'avg_session_time' in X_base.columns:\n",
    "    X_base['engagement_score'] = X_base['total_sessions'] * X_base['avg_session_time']\n",
    "    interaction_count += 1\n",
    "\n",
    "if 'days_active_ratio' in X_base.columns and 'total_sessions' in X_base.columns:\n",
    "    X_base['active_sessions_ratio'] = X_base['days_active_ratio'] * X_base['total_sessions']\n",
    "    interaction_count += 1\n",
    "\n",
    "# Interacciones de revenue hist√≥rico\n",
    "if 'iap_revenue_usd_bundle_sum' in X_base.columns and 'num_buys_bundle_count' in X_base.columns:\n",
    "    X_base['avg_revenue_per_purchase'] = X_base['iap_revenue_usd_bundle_sum'] / (X_base['num_buys_bundle_count'] + 1)\n",
    "    interaction_count += 1\n",
    "\n",
    "if 'whale_users_bundle_total_revenue_sum' in X_base.columns and 'user_bundles_count' in X_base.columns:\n",
    "    X_base['whale_revenue_per_bundle'] = X_base['whale_users_bundle_total_revenue_sum'] / (X_base['user_bundles_count'] + 1)\n",
    "    interaction_count += 1\n",
    "\n",
    "print(f\"‚úì {interaction_count} features de interacci√≥n creadas\")\n",
    "\n",
    "# 6/6: TARGET ENCODING para categ√≥ricas (si existen)\n",
    "print(\"\\n[4/5] Aplicando target encoding a variables categ√≥ricas...\")\n",
    "\n",
    "categorical_cols = X_base.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols) > 0 and y_buyer is not None:\n",
    "    from category_encoders import TargetEncoder\n",
    "    \n",
    "    # Solo aplicar a categ√≥ricas con cardinalidad manejable\n",
    "    cat_to_encode = []\n",
    "    for col in categorical_cols:\n",
    "        if X_base[col].nunique() < 1000:  # Evitar alta cardinalidad\n",
    "            cat_to_encode.append(col)\n",
    "    \n",
    "    if cat_to_encode:\n",
    "        # Train/test split para evitar leakage\n",
    "        X_temp_train, X_temp_val, y_temp_train, y_temp_val = train_test_split(\n",
    "            X_base[cat_to_encode], y_buyer, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        encoder = TargetEncoder(cols=cat_to_encode, smoothing=1.0)\n",
    "        encoder.fit(X_temp_train, y_temp_train)\n",
    "        \n",
    "        X_encoded = encoder.transform(X_base[cat_to_encode])\n",
    "        X_encoded.columns = [f'{col}_target_enc' for col in cat_to_encode]\n",
    "        \n",
    "        X_base = pd.concat([X_base, X_encoded], axis=1)\n",
    "        print(f\"‚úì {len(cat_to_encode)} columnas target-encoded\")\n",
    "else:\n",
    "    print(\"‚úì No hay columnas categ√≥ricas para encodear\")\n",
    "\n",
    "# FREQUENCY ENCODING (alternativa simple)\n",
    "print(\"\\n[5/5] Aplicando frequency encoding...\")\n",
    "\n",
    "freq_encoded = 0\n",
    "for col in categorical_cols[:5]:  # Solo top 5 para evitar overhead\n",
    "    if col in X_base.columns:\n",
    "        freq = X_base[col].value_counts(normalize=True)\n",
    "        X_base[f'{col}_freq'] = X_base[col].map(freq).fillna(0)\n",
    "        freq_encoded += 1\n",
    "\n",
    "print(f\"‚úì {freq_encoded} columnas frequency-encoded\")\n",
    "\n",
    "# ============================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DE PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Features originales:     {X_base.shape[1] - len(binning_features) - interaction_count - len(categorical_cols)*2}\")\n",
    "print(f\"+ Binning:               {len(binning_features)}\")\n",
    "print(f\"+ Interacciones:         {interaction_count}\")\n",
    "print(f\"+ Target encoding:       {len(cat_to_encode) if 'cat_to_encode' in locals() else 0}\")\n",
    "print(f\"+ Frequency encoding:    {freq_encoded}\")\n",
    "print(f\"‚îÄ\" * 80)\n",
    "print(f\"Total features finales:  {X_base.shape[1]}\")\n",
    "print(f\"Total samples:           {X_base.shape[0]:,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Asignar a X para compatibilidad con celdas siguientes\n",
    "X = X_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b76ee1-2178-479b-b81a-8f6661d1eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNDER SAMPLING CON PROPORCI√ìN 1:4 (nos quedamos 10% del total de datos)\n",
    "# ============================================\n",
    "ratio = 4\n",
    "buyers = X[y_buyer == 1]\n",
    "non_buyers = X[y_buyer == 0]\n",
    "n_non_buyers = ratio*len(buyers)\n",
    "non_buyers_sample = resample(non_buyers, n_samples = n_non_buyers, random_state = 22, replace = False)\n",
    "\n",
    "X = pd.concat([buyers, non_buyers_sample])\n",
    "y_buyer = y_buyer.loc[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "339912b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENTRENAMIENTO DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "Dataset final:\n",
      "  Features: 64\n",
      "  Samples:  1,653,239\n",
      "\n",
      "Train: 1,322,591 | Test: 330,648\n",
      "\n",
      "============================================================\n",
      "ETAPA 1: CLASIFICACI√ìN (buyer_d7)\n",
      "============================================================\n",
      "\n",
      "Train: 1,322,591 | Test: 330,648\n",
      "\n",
      "============================================================\n",
      "ETAPA 1: CLASIFICACI√ìN (buyer_d7)\n",
      "============================================================\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[187]\tvalid_0's auc: 0.722859\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[187]\tvalid_0's auc: 0.722859\n",
      "‚úÖ AUC: 0.7229\n",
      "\n",
      "============================================================\n",
      "ETAPA 2: REGRESI√ìN (iap_revenue_d7)\n",
      "============================================================\n",
      "‚úÖ AUC: 0.7229\n",
      "\n",
      "============================================================\n",
      "ETAPA 2: REGRESI√ìN (iap_revenue_d7)\n",
      "============================================================\n",
      "Compradores en train: 43,847\n",
      "Compradores en train: 43,847\n",
      "‚úÖ RMSE: $399.62\n",
      "‚úÖ MAE:  $23.16\n",
      "\n",
      "============================================================\n",
      "RESUMEN\n",
      "============================================================\n",
      "AUC:  0.7229\n",
      "RMSE: $399.62\n",
      "MAE:  $23.16\n",
      "\n",
      "‚úì Modelos guardados en ./models/\n",
      "‚úÖ RMSE: $399.62\n",
      "‚úÖ MAE:  $23.16\n",
      "\n",
      "============================================================\n",
      "RESUMEN\n",
      "============================================================\n",
      "AUC:  0.7229\n",
      "RMSE: $399.62\n",
      "MAE:  $23.16\n",
      "\n",
      "‚úì Modelos guardados en ./models/\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENTRENAMIENTO TWO-STAGE CON LIGHTGBM\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# X, y_buyer, y_revenue ya est√°n definidos en la celda anterior\n",
    "print(f\"\\nDataset final:\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Samples:  {X.shape[0]:,}\")\n",
    "\n",
    "# ============================================\n",
    "# TWO-STAGE MODEL: CLASIFICACI√ìN + REGRESI√ìN\n",
    "# ============================================\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_buyer_train, y_buyer_test, y_rev_train, y_rev_test = train_test_split(\n",
    "    X, y_buyer, y_revenue, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_buyer\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}\")\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 1: CLASIFICADOR (Buyer)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ETAPA 1: CLASIFICACI√ìN (buyer_d7)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "params_buyer = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'scale_pos_weight': (y_buyer_train == 0).sum() / (y_buyer_train == 1).sum()\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_buyer_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_buyer_test, reference=lgb_train)\n",
    "\n",
    "model_buyer = lgb.train(\n",
    "    params_buyer,\n",
    "    lgb_train,\n",
    "    num_boost_round=200,\n",
    "    valid_sets=[lgb_eval],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(period=0)]\n",
    ")\n",
    "\n",
    "y_buyer_pred = model_buyer.predict(X_test, num_iteration=model_buyer.best_iteration)\n",
    "auc = roc_auc_score(y_buyer_test, y_buyer_pred)\n",
    "\n",
    "print(f\"‚úÖ AUC: {auc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 2: REGRESOR (Revenue)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ETAPA 2: REGRESI√ìN (iap_revenue_d7)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Solo compradores\n",
    "X_train_buyers = X_train[y_buyer_train == 1]\n",
    "y_rev_train_buyers = y_rev_train[y_buyer_train == 1]\n",
    "# aplicamos transformaci√≥n y = log(1 + y) al target\n",
    "y_rev_train_buyers = np.log1p(y_rev_train_buyers)\n",
    "lgb_train_rev = lgb.Dataset(X_train_buyers, y_rev_train_buyers)\n",
    "\n",
    "print(f\"Compradores en train: {len(X_train_buyers):,}\")\n",
    "\n",
    "params_revenue = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "lgb_train_rev = lgb.Dataset(X_train_buyers, y_rev_train_buyers)\n",
    "\n",
    "model_revenue = lgb.train(\n",
    "    params_revenue,\n",
    "    lgb_train_rev,\n",
    "    num_boost_round=250,\n",
    "    callbacks=[lgb.log_evaluation(period=0)]\n",
    ")\n",
    "\n",
    "# Predicciones combinadas\n",
    "buyer_threshold = 0.3\n",
    "predicted_buyers = y_buyer_pred > buyer_threshold\n",
    "y_rev_pred = np.zeros(len(X_test))\n",
    "y_rev_pred[predicted_buyers] = model_revenue.predict( # debido a transformaci√≥n inicial predecimos expm1\n",
    "    X_test[predicted_buyers], \n",
    "    num_iteration=model_revenue.best_iteration\n",
    ")\n",
    "y_rev_pred_test = np.zeros(len(X_test_final))\n",
    "y_rev_pred_test[predicted_buyers] = np.expm1(model_revenue.predict(X_test[predicted_buyers]))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_rev_test, y_rev_pred))\n",
    "mae = mean_absolute_error(y_rev_test, y_rev_pred)\n",
    "\n",
    "print(f\"‚úÖ RMSE: ${rmse:.2f}\")\n",
    "print(f\"‚úÖ MAE:  ${mae:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC:  {auc:.4f}\")\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"MAE:  ${mae:.2f}\")\n",
    "\n",
    "# Guardar modelos\n",
    "model_buyer.save_model('./models/buyer_classifier_dask.txt')\n",
    "model_revenue.save_model('./models/revenue_regressor_dask.txt')\n",
    "print(\"\\n‚úì Modelos guardados en ./models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b56ac3",
   "metadata": {},
   "source": [
    "# ü§ñ PASO 4: Entrenar Modelo Two-Stage\n",
    "\n",
    "- Train/test split\n",
    "- Entrena clasificador LightGBM (buyer_d7)\n",
    "- Entrena regresor LightGBM (iap_revenue_d7)\n",
    "- Guarda modelos en `./models/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daba1ad",
   "metadata": {},
   "source": [
    "# üéØ PASO 5: Generar Predicciones y Submission\n",
    "\n",
    "- Carga test procesado\n",
    "- Alinea features con train\n",
    "- Genera predicciones\n",
    "- Crea `./outputs/submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf77a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESAMIENTO TEST SET - VERSI√ìN DASK\n",
      "================================================================================\n",
      "\n",
      "[1/4] Cargando test set...\n",
      "‚úì Test shape: 96 particiones\n",
      "‚úì row_id preservado como columna\n",
      "\n",
      "[2/4] Feature engineering...\n",
      "‚úì 4 features b√°sicas a√±adidas\n",
      "\n",
      "[3/4] Procesando columnas object...\n",
      "  ‚ö†Ô∏è  iap_revenue_usd_bundle ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  num_buys_bundle ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  whale_users_bundle_total_revenue ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  user_bundles ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  country_hist ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "‚úì row_id incluido en features guardadas\n",
      "\n",
      "Guardando test procesado...\n",
      "‚úì Test shape: 96 particiones\n",
      "‚úì row_id preservado como columna\n",
      "\n",
      "[2/4] Feature engineering...\n",
      "‚úì 4 features b√°sicas a√±adidas\n",
      "\n",
      "[3/4] Procesando columnas object...\n",
      "  ‚ö†Ô∏è  iap_revenue_usd_bundle ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  num_buys_bundle ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  whale_users_bundle_total_revenue ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  user_bundles ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "  ‚ö†Ô∏è  country_hist ‚Üí Error: name 'extract_array_stats_simple' is not\n",
      "‚úì row_id incluido en features guardadas\n",
      "\n",
      "Guardando test procesado...\n",
      "\n",
      "‚úì Test procesado y guardado en disco\n",
      "‚úì Test guardado en: ./processed_test_data\n",
      "\n",
      "‚úì Test procesado y guardado en disco\n",
      "‚úì Test guardado en: ./processed_test_data\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PROCESAR TEST SET CON DASK (VERSI√ìN OPTIMIZADA)\n",
    "# ============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESAMIENTO TEST SET - VERSI√ìN DASK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# PASO 1: CARGAR TEST\n",
    "# ============================================\n",
    "print(\"\\n[1/4] Cargando test set...\")\n",
    "\n",
    "test_path = \"./smadex-challenge-predict-the-revenue/test/test\"\n",
    "ddf_test = dd.read_parquet(test_path)\n",
    "\n",
    "print(f\"‚úì Test shape: {ddf_test.npartitions} particiones\")\n",
    "\n",
    "# IMPORTANTE: Convertir index a columna para preservar row_id\n",
    "if 'row_id' not in ddf_test.columns:\n",
    "    ddf_test = ddf_test.reset_index()\n",
    "    if 'index' in ddf_test.columns:\n",
    "        ddf_test = ddf_test.rename(columns={'index': 'row_id'})\n",
    "\n",
    "print(f\"‚úì row_id preservado como columna\")\n",
    "\n",
    "# ============================================\n",
    "# PASO 2: FEATURE ENGINEERING (LAZY)\n",
    "# ============================================\n",
    "print(\"\\n[2/4] Feature engineering...\")\n",
    "\n",
    "if 'avg_act_days' in ddf_test.columns and 'weeks_since_first_seen' in ddf_test.columns:\n",
    "    ddf_test['days_active_ratio'] = ddf_test['avg_act_days'] / (ddf_test['weeks_since_first_seen'] * 7 + 1e-10)\n",
    "\n",
    "if 'weekend_ratio' in ddf_test.columns:\n",
    "    ddf_test['is_weekend_user'] = (ddf_test['weekend_ratio'] > 0.5).astype(int)\n",
    "\n",
    "if 'wifi_ratio' in ddf_test.columns:\n",
    "    ddf_test['is_wifi_user'] = (ddf_test['wifi_ratio'] > 0.7).astype(int)\n",
    "\n",
    "if 'last_buy' in ddf_test.columns:\n",
    "    ddf_test['has_previous_purchase'] = (ddf_test['last_buy'] > 0).astype(int)\n",
    "\n",
    "print(\"‚úì 4 features b√°sicas a√±adidas\")\n",
    "\n",
    "# ============================================\n",
    "# PASO 3: PROCESAR COLUMNAS OBJECT (LAZY)\n",
    "# ============================================\n",
    "print(\"\\n[3/4] Procesando columnas object...\")\n",
    "\n",
    "# Procesar solo top 5 columnas (igual que en train)\n",
    "top_priority_cols_test = [\n",
    "    'iap_revenue_usd_bundle',\n",
    "    'num_buys_bundle',\n",
    "    'whale_users_bundle_total_revenue',\n",
    "    'user_bundles',\n",
    "    'country_hist'\n",
    "]\n",
    "\n",
    "cols_to_process_test = [col for col in top_priority_cols_test if col in ddf_test.columns]\n",
    "\n",
    "for col in cols_to_process_test:\n",
    "    try:\n",
    "        count, total_sum, total_max = extract_array_stats_simple(ddf_test[col], col)\n",
    "        \n",
    "        ddf_test[f'{col}_count'] = count\n",
    "        ddf_test[f'{col}_sum'] = total_sum\n",
    "        ddf_test[f'{col}_max'] = total_max\n",
    "        \n",
    "# PASO 4: SELECCIONAR FEATURES NUM√âRICAS + ROW_ID\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  {col} ‚Üí Error: {str(e)[:40]}\")\n",
    "\n",
    "# ============================================\n",
    "# PASO 4: SELECCIONAR FEATURES NUM√âRICAS\n",
    "\n",
    "# IMPORTANTE: Asegurar que row_id est√© incluido\n",
    "if 'row_id' in ddf_test.columns and 'row_id' not in numeric_cols_test:\n",
    "    numeric_cols_test.append('row_id')\n",
    "\n",
    "ddf_test_numeric = ddf_test[numeric_cols_test]\n",
    "\n",
    "print(f\"‚úì row_id incluido en features guardadas\")\n",
    "# PASO 5: GUARDAR TEST PROCESADO\n",
    "# ============================================\n",
    "# PASO 5: GUARDAR TEST PROCESADO CON ROW_ID\n",
    "# ============================================\n",
    "print(\"\\nGuardando test procesado...\")\n",
    "output_test_path = './processed_test_data'\n",
    "ddf_test_numeric.to_parquet(output_test_path, compression='snappy', overwrite=True, write_index=False)\n",
    "\n",
    "print(\"\\n‚úì Test procesado y guardado en disco\")\n",
    "gc.collect()\n",
    "del ddf_test\n",
    "# Liberar memoria\n",
    "print(f\"‚úì Test guardado en: {output_test_path}\")# Liberar memoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERANDO PREDICCIONES FINALES\n",
      "================================================================================\n",
      "\n",
      "[1/3] Cargando test procesado...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GENERAR PREDICCIONES Y SUBMISSION\n",
    "# ============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERANDO PREDICCIONES FINALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# PASO 1: CARGAR TEST PROCESADO\n",
    "# ============================================\n",
    "print(\"\\n[1/3] Cargando test procesado...\")\n",
    "\n",
    "ddf_test_processed = dd.read_parquet('./processed_test_data')\n",
    "\n",
    "# Computar en chunks o completo seg√∫n RAM disponible\n",
    "df_test = ddf_test_processed.compute()\n",
    "\n",
    "print(f\"‚úì Test cargado: {df_test.shape}\")\n",
    "\n",
    "# IMPORTANTE: Extraer y preservar row_id\n",
    "if 'row_id' in df_test.columns:\n",
    "    row_ids = df_test['row_id'].values\n",
    "    print(f\"‚úì row_id extra√≠do: {len(row_ids):,} registros\")\n",
    "    print(f\"‚úì IDs √∫nicos: {len(set(row_ids)):,}\")\n",
    "    \n",
    "    # Verificar duplicados\n",
    "    if len(row_ids) != len(set(row_ids)):\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Se encontraron {len(row_ids) - len(set(row_ids))} IDs duplicados\")\n",
    "    \n",
    "    # Eliminar row_id de las features\n",
    "    df_test = df_test.drop(columns=['row_id'])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No se encontr√≥ row_id en el dataset\")\n",
    "    row_ids = None\n",
    "\n",
    "# ============================================\n",
    "# PASO 2: ALINEAR FEATURES CON TRAIN\n",
    "# ============================================\n",
    "print(\"\\n[2/3] Alineando features con train...\")\n",
    "\n",
    "# Asegurar que tenemos las mismas columnas que en train\n",
    "X_test_final = pd.DataFrame(index=df_test.index)\n",
    "\n",
    "for col in X.columns:\n",
    "    if col in df_test.columns:\n",
    "        X_test_final[col] = df_test[col]\n",
    "    else:\n",
    "        # Si falta una feature, rellenar con 0\n",
    "        X_test_final[col] = 0\n",
    "        print(f\"  ‚ö†Ô∏è  Missing: {col} ‚Üí Filled with 0\")\n",
    "\n",
    "print(f\"‚úì Features alineadas: {X_test_final.shape[1]}\")\n",
    "\n",
    "# ============================================\n",
    "# PASO 3: GENERAR PREDICCIONES\n",
    "# ============================================\n",
    "print(\"\\n[3/3] Generando predicciones...\")\n",
    "\n",
    "# Cargar modelos guardados\n",
    "model_buyer = lgb.Booster(model_file='./models/buyer_classifier_dask.txt')\n",
    "model_revenue = lgb.Booster(model_file='./models/revenue_regressor_dask.txt')\n",
    "\n",
    "# Predicci√≥n de compradores\n",
    "y_buyer_pred_test = model_buyer.predict(X_test_final)\n",
    "\n",
    "# Identificar compradores potenciales\n",
    "buyer_threshold = 0.3\n",
    "predicted_buyers = y_buyer_pred_test > buyer_threshold\n",
    "\n",
    "# Predicci√≥n de revenue (solo para compradores)\n",
    "y_revenue_pred_test = np.zeros(len(X_test_final))\n",
    "\n",
    "if predicted_buyers.sum() > 0:\n",
    "    y_revenue_pred_test[predicted_buyers] = model_revenue.predict(X_test_final[predicted_buyers])\n",
    "\n",
    "# Verificar que row_ids existe y tiene el tama√±o correcto\n",
    "if row_ids is None:\n",
    "    raise ValueError(\"‚ùå ERROR: row_ids no est√° disponible. Ejecuta la celda de procesamiento de test.\")\n",
    "\n",
    "if len(row_ids) != len(y_revenue_pred_test):\n",
    "    raise ValueError(f\"‚ùå ERROR: Mismatch en tama√±os - row_ids: {len(row_ids)}, predicciones: {len(y_revenue_pred_test)}\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': row_ids,\n",
    "    'iap_revenue_d7': y_revenue_pred_test\n",
    "})\n",
    "\n",
    "# Verificar duplicados antes de guardar\n",
    "duplicates = submission['row_id'].duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {duplicates} IDs duplicados encontrados - eliminando duplicados...\")\n",
    "    submission = submission.drop_duplicates(subset=['row_id'], keep='first')\n",
    "\n",
    "# Guardar\n",
    "output_file = './outputs/submission.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo: {output_file}\")\n",
    "print(f\"‚úÖ Registros: {len(submission):,}\")\n",
    "print(f\"\\nPrimeras 10 predicciones:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ ¬°LISTO PARA KAGGLE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f690c27",
   "metadata": {},
   "source": [
    "# üìä CONFIGURACI√ìN Y T√âCNICAS DE PREPROCESSING\n",
    "\n",
    "## üé® T√©cnicas Implementadas\n",
    "\n",
    "### ‚úÖ 1. Feature Engineering B√°sico\n",
    "```python\n",
    "# Ratios y proporciones\n",
    "days_active_ratio = avg_act_days / (weeks_since_first_seen * 7)\n",
    "avg_sessions_per_week = total_sessions / weeks_since_first_seen\n",
    "\n",
    "# Flags comportamentales\n",
    "is_new_user = (weeks_since_first_seen < 1)\n",
    "is_veteran_user = (weeks_since_first_seen > 12)\n",
    "recent_buyer = (last_buy < 7)\n",
    "```\n",
    "\n",
    "### ‚úÖ 2. Extracci√≥n de Object Columns\n",
    "Convierte listas/dicts en features num√©ricas:\n",
    "- `count`: N√∫mero de elementos\n",
    "- `sum, max, min, mean, std`: Estad√≠sticas agregadas\n",
    "\n",
    "### ‚úÖ 3. Binning (Discretizaci√≥n)\n",
    "```python\n",
    "# Variables continuas ‚Üí Categor√≠as\n",
    "user_age_bin = pd.qcut(weeks_since_first_seen, q=5)\n",
    "sessions_bin = pd.qcut(total_sessions, q=5)\n",
    "session_time_bin = pd.qcut(avg_session_time, q=5)\n",
    "```\n",
    "**Ventaja**: Captura relaciones no lineales, reduce outliers\n",
    "\n",
    "### ‚úÖ 4. Features de Interacci√≥n\n",
    "```python\n",
    "# Combina m√∫ltiples features\n",
    "engagement_score = total_sessions * avg_session_time\n",
    "active_sessions_ratio = days_active_ratio * total_sessions\n",
    "avg_revenue_per_purchase = iap_revenue_sum / (num_buys + 1)\n",
    "```\n",
    "**Ventaja**: Captura sinergias entre variables\n",
    "\n",
    "### ‚úÖ 5. Target Encoding\n",
    "```python\n",
    "# Codifica categor√≠as con media del target\n",
    "country_encoded = mean(target | country)\n",
    "```\n",
    "**Ventaja**: Mejor que label encoding para alta cardinalidad\n",
    "**Cuidado**: Incluye smoothing y train/val split para evitar leakage\n",
    "\n",
    "### ‚úÖ 6. Frequency Encoding\n",
    "```python\n",
    "# Codifica por frecuencia de aparici√≥n\n",
    "country_freq = count(country) / total_rows\n",
    "```\n",
    "**Ventaja**: Simple, r√°pido, sin leakage\n",
    "\n",
    "## üîß Par√°metros Configurables\n",
    "\n",
    "### Rango de Fechas (Celda 3)\n",
    "```python\n",
    "# 14 d√≠as completos\n",
    "filters = [(\"datetime\", \">=\", \"2025-10-06-00-00\"), (\"datetime\", \"<\", \"2025-10-20-00-00\")]\n",
    "\n",
    "# Solo 1 d√≠a (pruebas r√°pidas)\n",
    "filters = [(\"datetime\", \">=\", \"2025-10-06-00-00\"), (\"datetime\", \"<\", \"2025-10-07-00-00\")]\n",
    "```\n",
    "\n",
    "### Muestreo (Celda 7)\n",
    "```python\n",
    "sample_fraction = 0.5  # 50% (recomendado 16GB RAM)\n",
    "sample_fraction = 0.8  # 80% (m√°s datos, m√°s RAM)\n",
    "sample_fraction = 0.3  # 30% (menos memoria)\n",
    "```\n",
    "\n",
    "### Columnas Object (Celda 5)\n",
    "A√±ade m√°s columnas a `top_priority_cols`:\n",
    "```python\n",
    "top_priority_cols = [\n",
    "    'iap_revenue_usd_bundle',\n",
    "    'num_buys_bundle',\n",
    "    'whale_users_bundle_total_revenue',\n",
    "    'user_bundles',\n",
    "    'country_hist',\n",
    "    'advertiser_actions_action_count',  # Ya incluida\n",
    "    'user_actions_bundles_action_count'  # Ya incluida\n",
    "]\n",
    "```\n",
    "\n",
    "## üí° Pr√≥ximas Mejoras (Roadmap)\n",
    "\n",
    "### üéØ Alta Prioridad\n",
    "- [ ] **RFM Features**: Recency, Frequency, Monetary para comportamiento de compra\n",
    "- [ ] **Agregaciones por grupo**: Mean/max/std por country, carrier, dev_os\n",
    "- [ ] **Embeddings categ√≥ricos**: Entity embeddings para alta cardinalidad\n",
    "- [ ] **Time-based features**: Hour, day_of_week, is_weekend del datetime\n",
    "\n",
    "### üî¨ Media Prioridad\n",
    "- [ ] **Feature selection**: Eliminar features redundantes (correlaci√≥n > 0.95)\n",
    "- [ ] **Normalizaci√≥n**: StandardScaler para features num√©ricas\n",
    "- [ ] **Polynomial features**: Interacciones de orden 2\n",
    "- [ ] **Ensemble**: Combinar XGBoost + LightGBM + CatBoost\n",
    "\n",
    "### ‚ö° Baja Prioridad (optimizaci√≥n)\n",
    "- [ ] **Hyperparameter tuning**: Optuna para optimizar LightGBM\n",
    "- [ ] **Cross-validation**: 5-fold CV para estimaci√≥n m√°s robusta\n",
    "- [ ] **Feature importance**: SHAP values para interpretabilidad\n",
    "\n",
    "## üìÅ Archivos Generados\n",
    "\n",
    "| Archivo | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| `./processed_train_data/` | Train procesado (Parquet) |\n",
    "| `./processed_test_data/` | Test procesado (Parquet) |\n",
    "| `./models/buyer_classifier_dask.txt` | Clasificador LightGBM |\n",
    "| `./models/revenue_regressor_dask.txt` | Regresor LightGBM |\n",
    "| `./outputs/submission.csv` | Submission Kaggle |\n",
    "\n",
    "## ‚ö†Ô∏è Troubleshooting\n",
    "\n",
    "**Memory full**: Reducir `sample_fraction` o procesar menos object columns\n",
    "\n",
    "**Categorical encoding error**: Verificar que las categor√≠as existen en train y test\n",
    "\n",
    "**Predicciones = 0**: Revisar `buyer_threshold` y que los modelos se guardaron correctamente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
